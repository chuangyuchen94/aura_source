### 1. 线性回归方程实现
- 用最小二乘法求解参数值$\theta$，并进行预测 $$\theta=(X^TX)^{-1}X^Ty$$
  - 训练（建模）
    - X的构造
      - 在样本的基础上，在最前面增加一列、值全为1，代表偏置项
    - $(X^TX)^{-1}$的意思：X的转置乘以X，再求逆
      - 求逆：np.linalg.inv(np.dot(X.T,X))
  - 预测：$$h_\theta(X)=\theta^Tx=X\theta$$
  - 局限性
    - 不是所有的数据都可以求逆
    - 没有体现机器学习的过程
- sklearn工具包：LinearRegression

### 2. 梯度下降效果
- 对算法进行训练时，是怎么做的：逐步进行学习
  - 随机找到一个位置，选择一个合适的方向，往这个方向走
- 可能会遇到的问题
  - 问题1：步长太小。导致收敛慢，训练花费时间多
  - 问题2：步长太大，导致训练结果跌宕起伏，训练结果不准确
  - 问题3：局部的最低点。其实是要寻找全局最低点
- 标准化的作用
  - 不同的特征，取值范围不同：越大的取值范围，数值的浮动（方差）也就越大
  - 如果不同的特征的取值范围不同，且有些特征的取值范围很大，会导致收敛速度很慢
  - 因此，在拿到数据后，首先要做的就是对其进行标准化
- 标准工具包：sklearn.preprocessing
  - 方法很多，需要根据实际遇到的情况进行选择
- 批量梯度下降
  - 公式：$$\theta := \theta -\alpha\frac{1}{m}X^{T}(X\theta-y)$$
- 随机梯度下降
  - 在迭代过程中，随机选择一个样本，用于梯度下降，同时，将所有的样本的每一个都用于这个过程
  - 在计算梯度时，不用做$1/m$
- 小批量梯度下降：结合了批量梯度下降和随机梯度下降
  - 先固定个数、随机取出小批量，再对小批量逐笔执行训练过程
- 学习率对结果的影响
  - 选择学习率时，宁可学习率小、训练次数多，也不选择学习率大，避免学习跑偏
  - 学习率的衰减策略：一开始可以大一点，随着学习的进行、越来越接近目标，学习率应该越来越小

### 3. 对比不同梯度下降策略
- 结论：一般来说，常用的是小批量梯度下降
  - 批处理数量的大小，32、64、128都可以，需要同时考虑内存和效率

### 4. 建模曲线分析
- 工具：
  - sklearn.preprocessing.PolynomialFeatures # 用于生成多项式特征
    - 先实例化PolynomialFeatures类，再调用fit_transform方法
  - sklearn.linear_model.LinearRegression # 也可用于多项式回归
- 启发：
  - degree过高，容易导致过拟合，一般degree为1~3就够用

### 5. 过拟合与欠拟合
- 测试集数量对结果的影响
  - 数据量越大，过拟合的风险越低
- 对线性回归的结果进行评估
  - 工具--均方误差：sklearn.metrics.mean_squared_error

### 6. 正则化的作用


### 7. 提前停止策略

