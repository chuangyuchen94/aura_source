### 1. 算法的实现
- 第1步：初始化参数
  - 层数：神经元的层数
  - 权重参数：初始化（需要考虑偏置项）
  - 最大迭代次数
  - 学习率
- 第2步：数据预处理
  - 标准化
- 第3步：训练数据
  - 梯度下降：实现前向传播、反向传播
    - 计算当前的损失值
    - 计算当前的梯度值
    - 更新参数值
  - 损失函数：计算损失值
    - 前向传播得到最终结果，
    - 与真实值相比，计算损失值
  - 计算梯度值
    - 反向传播，
    - 每一层对结果的影响
    - 计算梯度值

### 2. 总结
- 前向传播、在计算损失值之前，是不知道最终结果的，即，训练的标签值，只会在计算损失值时用到，在训练、前向传播的过程中，并没有用到
- 前向传播结束后，计算得到损失值及梯度值，梯度值用于反向逐层更新参数
  - 输入层到第1层隐藏层：线性变换、sigmoid转换
  - 第1层隐藏层到输出层：线性变换、softmax计算概率值
  - 这种情况下，前向传播的梯度值如何计算，反向传播该如何做
- 链式法则
  - 隐藏层的误差：通过链式法则反向传播（假设包括输入层、输出层，总共有4层，输入层为第1层，输出层为第4层）
    - 输出层误差：输出层误差 = 输出层结果 - 输出层标签值 $$\delta^{(4)}=a^{(4)}-y$$
      - $\delta$：误差
      - $a$：输出层结果（预测值）
    - 反向传播：隐藏层误差 $$\delta^{(3)}={\theta^{(3)}}^T\delta^{(4)}\circ g'(z^{(3)})$$ $$\delta^{(2)}={\theta^{(2)}}^T\delta^{(3)}\circ g'(z^{(2)})$$
      - $\delta^{(l)}$：第$l$层的误差
      - ${(\theta^{(l)})}^T$：权重矩阵的转置
      - $g'(z^{(l)})$：激活函数的导数
        - Sigmoid函数的导数：$$g'(z^{(l)})=g(z)(1-g(z))$$ $$g(z)=\frac{1}{(1+e^{-z})}$$