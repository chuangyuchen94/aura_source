### 1. 主成分分析（PCA）
- 用途：
  - 降维中最常用的一种手段，比LDA还常用
- 目标
  - 提取最有价值的信息（基于方差）
    - 哪些特征，能够使得数据更分明
- 无监督

### 2. 向量的表示及基变换
- 内积：$$(a_1,a_2,...,a_n)^T · (b_1,b_2,...,b_n)=a_1b_1+a_2b_2+...+a_nb_n$$
- 解释：$$A·B=|A||B|cos(a)$$
  - $a$为A与B之间的夹角
- 基变换
  - 基是正交的，即，内积为0，

### 3. 基本概念
- 协方差矩阵
  - 定义：特征之间相关的程度
  - 方向：如何选择方向（或者，基），才能尽量保留最多的原始信息
    - 投影后的投影值，尽可能分散
  - `方差`：$$Var(a)=\frac{1}{m}\sum_{i=1}^{m}(a_i-\mu)^2$$
    - 如果只是单纯选择方差最大的方向，后续选择的方向，会和方差最大的方向接近重合
    - 解决方案：选择的方向之间，尽量不存在（线性）相关性
  - `协方差`：表示两个字段的相关性 $$Cov(a, b)=\frac{1}{m}\sum_{i=1}^ma_ib_i$$
    - 用矩阵运算表示：$$\frac{1}{m}X·X^T$$
    - 当协方差为0时，表示两个字段完全独立；
    - 为了让协方差为0，选择第二个基时，只能在与第一个基正交的方向上选择

### 4. 优化目标
- 将一组N维向量降为K维（K大于0，小于N），目标是选择K个单位正交基，使原始数据变换到这组基上后，各字段两两间协方差为0，字段的方差则尽可能大
- 协方差矩阵对角化
  - 即，除对角线外的其它元素化为0，并且在对角线上，将元素按大小从上到下排列
  - 协方差矩阵对角化：$$
PCP^T = Λ = \begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{pmatrix}
$$
  - 实对称矩阵：一个(n, n)的实对称矩阵，一定可以找到n个单位正交的特征向量$$E=(e_1,e_2,...,e_n)$$
  - 实对称矩阵，可进行对角化 $$E^TCE= Λ = \begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{pmatrix}$$
  - 根据特征值从大到小，将特征向量从上到下排列；用前K行组成的矩阵，乘以原始数据矩阵X，就得到了降维后的数据矩阵Y

### 5. 实例
- 步骤1：数据预处理（在拿到数据之后，一般不会直接就进行降维操作）
  - 标准化：from sklearn.preprocessing import StandardScaler
- 步骤2：计算协方差矩阵
  - 协方差：特征之间的相关性
- 步骤3：计算特征值、特征向量
- 步骤4：计算cumsum值，根据阈值来选择特征
- 