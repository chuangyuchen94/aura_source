### 1. 词向量模型word2vector
- 文本输入不能直接输入到机器学习模型当中
  - 一个句子中，词之间有前后顺序
  - 同义词
  - 近义词
- 与神经网络思路相似：用神经网络来做
- 如何将文本向量化：将词转换为词向量
  - 词向量维度较高：50~300
    - 通常，数据的维度越高，能提供的信息就越多，从而计算的结果的可靠性就越高
  - 有了向量，就可以评估相似度
    - 用余弦相似度或欧式距离来表达
- 如何训练词向量模型
  - 输入
    - 从词库中转为向量
  - 输出：根据输入，猜出下一个词是什么
    - 分类任务（softmax）
  - 训练模型的数据从哪里来
    - 一切可用的、正常的文本，都可以当做训练数据

### 2. 构建训练数据
- 滑动窗口：每次取固定个数的单词作为输入
- 移动滑动窗口，来构建输入和输出
  - 每次往前移动一个词
  - 输出为当前滑动窗口的下一个单词

### 3. 训练模型
- 不同架构的模型
  - CBOW
    - 滑动窗口为5个单词
    - 输入为前面2个单词，加后面两个单词，构成上下文
    - 输出为中间的1个单词
    - 构建的数据集：
  - Skip-gram模型
    - 通过给定一个词，来预测上下文
    - 在滑动窗口中，中间的词为输入，上下文为输出
    - 构建的数据集：
      - 构建输入与输出的每个词的训练集：(输入-单词3, 输出-单词1), (输入, 输出-单词2),(输入, 输出-单词4), (输入, 输出-单词5),   

### 4. 训练过程
- 前向传播、反向传播
  - 反向传播的最后，还要更新输入的词向量
- 如果语料库很大，可能的结果就很多，最后一层相当于softmax，计算很耗时，如何解决
  - 优化思路：
    - 将多分类任务，转化为二分类任务
      - 把输出也作为模型的输入，让模型根据输入和输出，计算出数值，如果是上下文，应该越接近1越好
    - 加入负样本（负采样模型）：
      - 加入一些非上下文的输出数据，预测结果应该越接近0越好；
      - 负样本是人为构建出来的
      - 负样本个数：5个比较合适

### 5. 词向量完整的训练过程
- 第1步：初始化词向量矩阵
  - lookup embedding初始化词向量（随机初始化）
- 第2步：通过神经网络反向传播来计算更新，同时更新权重参数矩阵W，还更新输入数据X

### 6. 用tensorflow来实现词向量模型
- 语料的选择：任何逻辑正常的文本都可以
- 第1步：构建数据（输入、输出）
  - 选择模型：skip-gram
  - 滑动窗口：5个单词
- 第2步：将单词转换为特征
  - 特征维度：300
  - 取网上已经训练好的词向量模型
    - 构建常见的词向量模型
- 第3步：找出常用的5W个词，其他词标记为UKN（unknown）
- 第4步：batch数据制作
- 第5步：网络训练
  - 最终输出时，softmax如何提速
  - loss：NCE
  - 