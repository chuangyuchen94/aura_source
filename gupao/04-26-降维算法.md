### 1. 线性判别分析（LDA）
- 为什么要做降维
  - 高维：数据的特征多
    - 维度大，需要的计算量也大
  - 不是所有的特征都有用
    - 有的特征重要，有的特征不重要
- LDA：Linear Discriminant Analysis
  - 用途：
    - 数据预处理中的降维
    - 分类任务
  - 目标：最大化类间区分度的坐标轴成分
    - 将特征空间（由数据集组成的多维空间）`投影`到一个维度更小的K维子空间，同时，保持区分类别的信息
  - 原理
    - 投影到维度更低的空间中，使得投影后的点，会形成按类别区分的、一簇一簇的情况；相同类别的点，在投影后的空间中，会更加接近
  - 监督性：LDA是有监督的，计算的是另一类特定的方向
  - `投影`：找到更合适分类的空间
  - 与PCA不同
    - LDA更关心分类，而不是方差
    - PCA不是有监督
 
### 2. 数学原理及目标函数
- `目标：找到投影`$$y=w^Tx$$
  - $x$：原始的数据的矩阵
  - $w$：投影映射变换的矩阵
  - $y$：投影后的数据的矩阵
  - 将目标变为找$w$
- `目标的具体化定义`：使得不同类别之间的距离越远越好，相同类别之中的距离越近越好
  - 目标1：最大化不同类别之间的距离
    - 每类样例的均值：$$\mu_i=\frac{1}{N_i}\sum_{x \in w_i}x$$
    - 投影后的均值：$$\tilde\mu_i=\frac{1}{N_i}\sum_{y \in w_i}y=\frac{1}{N_i}\sum_{x \in w_i}w^Tx=w^T\mu_i$$
    - 投影后，两类样本的中心点尽量分离：$$J(w)=|\tilde\mu_1-\tilde\mu_2|=|w^T(\mu_1-\mu_2)|$$
  - 目标2：最小化相同类别之中的距离
    - 散列值：样本点的密集程度——值越大，越分散；值越小，越密集
    - 同类别之间，应该越密集：$$\tilde s_i^2=\sum_{y \in w_i}(y - \tilde\mu_i)^2$$
  - 进一步的，合并目标1和目标2，得到`目标函数`：$$J(w)=\frac{|\tilde\mu_1-\tilde\mu_2|^2}{\tilde s_1^2+\tilde s_2^2}$$
- 将目标函数展开，得到最终目标函数：$$J(w)=\frac{w^TS_Bw}{w^TS_ww}$$
  - $S_B$：类间散布矩阵
  - $S_w$：类内散布矩阵
- 分母进行归一化：将分母限制为1，否则，分子、分母可以取任意值，使得$w$有无穷解
- 在已知条件的约束下，求解目标函数：用拉格朗日乘子法，最终得到：$$S_w^{-1}S_Bw=\lambda w$$
  - $w$是矩阵$S_w^{-1}S_B$的特征向量

### 3. 实现LDA算法
- 数据文件：https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
- 过程
  - 步骤1：读取数据，并将标签值转换为数值表示
    - 工具：
      - import pandas as pd # 读取数据，网页上的
      - from sklearn.preprocessing import LabelEncoder # 将字母、单词的枚举值，转换成数值型的枚举值
  - 步骤2：求投影前的、每类样例的均值
    - $$m_i=\frac{1}{n_i}\sum_{x \in D_i}^nx_k$$
  - 步骤3：求类内散布矩阵$S_w$
    - $$S_w=\sum_{i=1}^cS_i$$
    - $$S_i=\sum_{x \in D_i}^n(x-m_i)(x-m_i)^T$$
  - 步骤4：求类间散布矩阵$S_B$
    - $$S_B=\sum_{i=1}^cN_i(m_i-m)(m_i-m)^T$$
      - $m$是全局的均值
      - $m_i$：每类样本的均值
      - $N_i$：每类样本的样本数
  - 步骤5：求$S_w^{-1}S_B$的特征向量和特征值
    - 特征向量：表示映射的方向（投影的方向）
    - 特征值：特征向量的重要程度，值越大，表示越重要
  - 步骤6：进行降维（减少维度）
    - 从特征值中，挑选出重要的特征向量（1列），作为降维后的映射的方向
      - 具体要减少的维度，需要根据业务场景来定
    - 将特征向量组合为矩阵，作为投影转换的矩阵$w$
    - 