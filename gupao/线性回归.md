### 概念
- 线性回归
  - 是一种有监督学习
  - 特征与标签
  - 回归方程
  - 误差：真实值与预测值之间是存在差异的，用损失函数来表示
    - 误差$\epsilon^{(i)}$是独立并且具有相同的分布，服从均值为0方差为$\theta^2$的高斯分布
    - 似然函数：什么样的参数跟我们的数据组合后恰好是真实值
  - 梯度下降
    - 线性回归一步步的完成迭代
    - 下山分几步走？（更新参数）
      - （1）找到当前最合适的方向
      - （2）走那么一小步，走快了该“跌到”了
      - （3）按照方向与步伐去更新我们的参数
    - 学习率（步长）：对结果会产生巨大的影响，一般小一些更好
      - 如何选择：从小的时候，不行再小
    - 批处理数量：32，64，128都可以，很多时候还得考虑内存和效率

### 1. 线性归回
- 线性归回，是为了找到自变量（x）与因变量（y）之间的线性关系的统计模型，同时，使得预测值与真实值之间的误差最小化
- 线性回归的数学公式：$y = \theta_0 + \theta_1x_1+ \theta_2x_2+...+\theta_nx_n$
- 将其用向量点乘的方式表示：$y = \theta^Tx$
  - 其中，$\theta$是参数向量，为列向量；x为特征向量，为列向量
    - $\theta=\begin{bmatrix} \theta_0 \\ \theta_1 \\ ...\\ \theta_n \\ \end{bmatrix}$, x = $\begin{bmatrix} x_0 \\ x_1 \\ ...\\ x_n \\ \end{bmatrix}$
  - $\theta^T$表示$\theta$的转置，转置之后为(1, n)的行向量
    - $\theta^T=\begin{bmatrix} \theta_0, & \theta_1, & ...& \theta_n \\ \end{bmatrix}$
  - $\theta^Tx$为(1, n)@(n, 1)，结果为(1, 1)的矩阵
- 考虑误差项$\varepsilon$，对于每个样本来说: $y^{(i)}=\theta^Tx{(i)} + \varepsilon^{(i)}$
  - 误差项$\varepsilon^{(i)}$是每个预测值与真实值之间的差值
  - 误差的统计假设
    - 独立性：不同样本的误差是相互独立的
    - 同分布：所有误差项都来自同一个概率分布，误差波动规律相同
    - 正态分布：服从均值为0，方差为$\sigma^2$的高斯分布
- 误差项服从高斯分布，符合概率密度函数：$p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma}}{e^{-\frac{({\varepsilon^{(i)}})^2}{2\sigma^2}}}$
- 代入概率密度函数：$p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y^{(i)-\theta^Tx^{(i)}})^2}{2\sigma^2}}$
- 构建`似然函数`：$L(\theta) = \prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)$ = $\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y^{(i)-\theta^Tx^{(i)}})^2}{2\sigma^2}}$
  - 似然函数：参数估计的核心方法，通过概率模型，找到最有可能生成观测数据的参数
  - 数据生成的过程，可以视为一个概率模型，符合高斯分布，描述了模型如何从潜在的分布中生成。模型需要定义两个核心部分：
    - 系统性部分：y = $\theta^Tx$
    - 随机性部分：$\varepsilon$, 符合正态分布的误差
  - 似然函数，就是在给定参数$\theta$时，观测数据出现的联合概率，这个概率自然是越大越好
- 对似然函数取对数：$\log L(\theta) = \sum_{i=1}^{m}\log\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y^{(i)-\theta^Tx^{(i)}})^2}{2\sigma^2}}$
- 展开化简：$\log L(\theta) = m\log \frac{1}{\sqrt{2\pi\sigma}}-\frac{1}{\sigma^2}·\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2$
- 让似然函数越大越好，等同于让$$\frac{1}{\sigma^2}·\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2$$越小越好，因此，似然函数的优化目标就是最小化这个`损失函数`, 即$$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2$$
- 