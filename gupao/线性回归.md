### 概念
- 线性回归
  - 是一种有监督学习
  - 特征与标签
  - 回归方程
  - 误差：真实值与预测值之间是存在差异的，用损失函数来表示
    - 误差$\epsilon^{(i)}$是独立并且具有相同的分布，服从均值为0方差为$\theta^2$的高斯分布
    - 似然函数：什么样的参数跟我们的数据组合后恰好是真实值
  - 梯度下降
    - 线性回归一步步的完成迭代
    - 下山分几步走？（更新参数）
      - （1）找到当前最合适的方向
      - （2）走那么一小步，走快了该“跌到”了
      - （3）按照方向与步伐去更新我们的参数
    - 学习率（步长）：对结果会产生巨大的影响，一般小一些更好
      - 如何选择：从小的时候，不行再小
    - 批处理数量：32，64，128都可以，很多时候还得考虑内存和效率

### 1. 线性归回
- 线性归回，是为了找到自变量（x）与因变量（y）之间的线性关系的统计模型，同时，使得预测值与真实值之间的误差最小化
- 线性回归的数学公式：$y = \theta_0 + \theta_1x_1+ \theta_2x_2+...+\theta_nx_n$
- 将其用向量点乘的方式表示：$y = \theta^Tx$
  - 其中，$\theta$是参数向量，为列向量；x为特征向量，为列向量
    - $\theta=\begin{bmatrix} \theta_0 \\ \theta_1 \\ ...\\ \theta_n \\ \end{bmatrix}$, x = $\begin{bmatrix} x_0 \\ x_1 \\ ...\\ x_n \\ \end{bmatrix}$
  - $\theta^T$表示$\theta$的转置，转置之后为(1, n)的行向量
    - $\theta=\begin{bmatrix} \theta_0, & \theta_1, & ...& \theta_n \\ \end{bmatrix}$
  - $\theta^Tx$为(1, n)@(n, 1)，结果为(1, 1)的矩阵
- 考虑误差项$\varepsilon$，对于每个样本来说: $y^{(i)}=\theta^Tx{(i)} + \varepsilon^{(i)}$
  - 误差项$\varepsilon^{(i)}$是每个预测值与真实值之间的差值
  - 误差的统计假设
    - 独立性：不同样本的误差是相互独立的
    - 同分布：所有误差项都来自同一个概率分布，误差波动规律相同
    - 正态分布：服从均值为0，方差为$\sigma^2$的高斯分布