### 概念
- 线性回归
  - 是一种有监督学习
  - 特征与标签
  - 回归方程
  - 误差：真实值与预测值之间是存在差异的，用损失函数来表示
    - 误差$\epsilon^{(i)}$是独立并且具有相同的分布，服从均值为0方差为$\theta^2$的高斯分布
    - 似然函数：什么样的参数跟我们的数据组合后恰好是真实值
  - 梯度下降
    - 线性回归一步步的完成迭代
    - 下山分几步走？（更新参数）
      - （1）找到当前最合适的方向
      - （2）走那么一小步，走快了该“跌到”了
      - （3）按照方向与步伐去更新我们的参数
    - 学习率（步长）：对结果会产生巨大的影响，一般小一些更好
      - 如何选择：从小的时候，不行再小
    - 批处理数量：32，64，128都可以，很多时候还得考虑内存和效率

### 1. 线性归回
- 线性归回，是为了找到自变量（x）与因变量（y）之间的线性关系的统计模型，同时，使得预测值与真实值之间的误差最小化
- 线性回归的数学公式：$y = \theta_0 + \theta_1x_1+ \theta_2x_2+...+\theta_nx_n$
- 将其用向量点乘的方式表示：$y = \theta^Tx$
  - 其中，$\theta$是参数向量，为列向量；x为特征向量，为列向量
    - $\theta=\begin{bmatrix} \theta_0 \\ \theta_1 \\ ...\\ \theta_n \\ \end{bmatrix}$, x = $\begin{bmatrix} x_0 \\ x_1 \\ ...\\ x_n \\ \end{bmatrix}$
  - $\theta^T$表示$\theta$的转置，转置之后为(1, n)的行向量
    - $\theta^T=\begin{bmatrix} \theta_0, & \theta_1, & ...& \theta_n \\ \end{bmatrix}$
  - $\theta^Tx$为(1, n)@(n, 1)，结果为(1, 1)的矩阵
- 考虑误差项$\varepsilon$，对于每个样本来说: $y^{(i)}=\theta^Tx{(i)} + \varepsilon^{(i)}$
  - 误差项$\varepsilon^{(i)}$是每个预测值与真实值之间的差值
  - 误差的统计假设
    - 独立性：不同样本的误差是相互独立的
    - 同分布：所有误差项都来自同一个概率分布，误差波动规律相同
    - 正态分布：服从均值为0，方差为$\sigma^2$的高斯分布
- 误差项服从高斯分布，符合概率密度函数：$p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma}}{e^{-\frac{({\varepsilon^{(i)}})^2}{2\sigma^2}}}$
- 代入概率密度函数：$p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y^{(i)-\theta^Tx^{(i)}})^2}{2\sigma^2}}$
- 构建`似然函数`：$L(\theta) = \prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)$ = $\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y^{(i)-\theta^Tx^{(i)}})^2}{2\sigma^2}}$
  - 似然函数：参数估计的核心方法，通过概率模型，找到最有可能生成观测数据的参数
  - 数据生成的过程，可以视为一个概率模型，符合高斯分布，描述了模型如何从潜在的分布中生成。模型需要定义两个核心部分：
    - 系统性部分：y = $\theta^Tx$
    - 随机性部分：$\varepsilon$, 符合正态分布的误差
  - 似然函数，就是在给定参数$\theta$时，观测数据出现的联合概率，这个概率自然是越大越好
- 对似然函数取对数：$\log L(\theta) = \sum_{i=1}^{m}\log\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y^{(i)-\theta^Tx^{(i)}})^2}{2\sigma^2}}$
- 展开化简：$\log L(\theta) = m\log \frac{1}{\sqrt{2\pi\sigma}}-\frac{1}{\sigma^2}·\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2$
- 让似然函数越大越好，等同于让$$\frac{1}{\sigma^2}·\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2$$越小越好，因此，似然函数的优化目标就是最小化这个`损失函数`, 即$$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2$$
  - 对损失函数的理解：使得整体的误差最小
- 进一步推演
  - 设$h_\theta(x^{(i)})=\theta^Tx^{(i)}$，是模型的预测值
  - 设计一个`(m, n+1)的矩阵X（在样本数据的基础上，最前面加一列，值为1）`，每一行是一个样本，每一列是一类特征值$$
X = 
\begin{bmatrix}
1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}
$$
  - 因此，$h_\theta(X)=\theta^Tx=X\theta$
  - `损失函数`：$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-h_\theta(x^{(i)}))^2=\frac{1}{2}(X\theta-y)^T(X\theta-y)$
    - $X\theta-y=\begin{bmatrix} h_\theta(x^{(1)})-y^{{1}} \\ h_\theta(x^{(2)})-y^{{2}} \\ ...\\ h_\theta(x^{(m)})-y^{{m}} \\ \end{bmatrix}$
- 求偏导：找到使损失函数最小的参数
  - 本质：利用`梯度下降法`来解决优化问题——`当梯度为0时，损失函数达到局部极值（可能是最小值）`
  - `偏导数`：用于描述，在多变量函数中，某个变量单独变化时（固定其他变量不变），函数值的变化率
  - `梯度`：是一个向量，表示损失函数关于每个参数$\theta$的偏导数$$
\nabla_\theta J(\theta) = \left[ \frac{\partial J}{\partial \theta_0}, \frac{\partial J}{\partial \theta_1}, \ldots, \frac{\partial J}{\partial \theta_n} \right]^T
$$
  - 梯度方向是损失函数的值在当前点上升最快的方向，负梯度方向是损失函数的值下降最快的方向
  - 通过沿着负梯度方向更新参数，可以逐步逼近损失函数的最小值
  - 展开损失函数：$J(\theta)=\frac{1}{2}(X\theta-y)^T(X\theta-y)=\frac{1}{2m}(\theta^TX^TX\theta-\theta^TX^Ty+y^Ty)$
    - 对$\theta_j$求偏导：$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)}
$$
    - 将所有参数的偏导数组成梯度向量：$$
\nabla_\theta J(\theta) = \frac{1}{m} X^T (X\theta - y)
$$
  - 令梯度为0，求解最优参数
    - 令$\nabla_\theta J(\theta) = 0$，得到$X^TX\theta=X^Ty$
    - $\theta=(X^TX)^{-1}X^Ty$
- 应用`梯度下降法`
  - 1）初始化参数$\theta$（设置学习率$\alpha$，设置最大迭代次数$max\_iter$，设置损失函数的阈值$threshold$）
    - 学习率的选择：尝试不同学习率（0.1，0.01，0.001等），结合损失曲线调整
  - 2）计算当前参数的梯度：$$\nabla_\theta J(\theta) = \frac{1}{m} X^T (X\theta - y)$$
  - 3）沿负梯度方向更新参数（当前梯度的绝对值乘以学习率，指明了参数可以调整的幅度）：$$\theta_j := \theta_j - \alpha \cdot \frac{\partial J}{\partial \theta_j}=\theta_j - \alpha \cdot\nabla_\theta J(\theta)$$
  - 4）重复步骤2~3，直到梯度接近于0，或达到最大迭代次数，或损失函数值小于阈值，此时的参数值，就是最优参数值
    - 梯度接近0的衡量：用梯度范数（即梯度向量的L2范数），np.linalg.norm($\nabla_\theta J(\theta)$)
    - 梯度爆炸/消失处理：
      - 若梯度范数突然增大（爆炸），需减小学习率或使用梯度裁剪（Gradient Clipping）。
      - 若梯度范数持续为零但损失未收敛，可能陷入鞍点，需检查模型或数据。
    - 阈值ϵ的调整(梯度小于该值时，可以认为已收敛)
      - 对高精度需求场景（如科学计算），可设ϵ=e−8
      - 对一般机器学习任务，ϵ=e-8已足够。