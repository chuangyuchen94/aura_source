### 1. 集成基本思想
- 测试时，对待测试样本分别通过不同的分类器，汇总最后的结果
- 投票策略：软投票，硬投票
  - 硬投票：少数服从多数，只看每个模型的结果，取出现次数大的作为最终结果
  - 软投票：看每个模型的值所对应的概率值，拿概率值及结果进行加权平均，作为最终结果
    - 要求各个分类器都能得出概率值
- 实验过程：
  - 先选择几个算法：
    - from sklearn.ensemble import VotingClassifier # 投票器
    - from sklearn.ensemble import RandomForestClassifier # 随机森林
    - from sklearn.linear_model import LogisticRegression # 逻辑回归
    - from sklearn.svm import SVC # 支持向量机
  - 各个分类算法实例化
    - RandomForestClassifier()
    - LogisticRegression()
    - SVC()
  - 投票器实例化：VotingClassifier
    - 参数：
      - estimators: 各个分类器对象组成的列表[("分类器的名字", 分类器对象), ("分类器的名字", 分类器对象), ...]
      - voting：投票策略
  - 用投票器实例进行训练
  - 用投票器进行预测
  - 衡量效果
    - from sklearn.metrics import accuracy_score

### 2. Bagging策略
- 算法过程
  - 首先，对训练数据集进行多次采样，保证每次得到的采样数据都是不同的(有放回的)
  - 然后，分别训练多个相同的模型（参数也是相同）
- 工具：
  - from sklearn.ensemble import BaggingClassifier # 集成算法
    - 参数
      - estimator：指定用什么树模型
      - n_estimators：用多少个模型实例
      - bootstrap：是否进行回放的采样
- OOB策略
  - OOB：Out Of Bag——袋外数据
  - BaggingClassifier：指定oob_score=True

### 3. 随机森林
- Bagging算法的典型代表
- 工具：from sklearn.ensenble import RandomForestClassifier
- 特征重要性：
  - RandomForestClassifier().feature_importances_
    - 数值越高，越重要
  - 用热度图更好的展示特征重要性

### 4. Boosting-提升策略
- AdaBoost
  - 样本权重项：每个数据的权重不是相同的
    - 每一轮训练后，没有验证对的数据，权重调高，验证对了的数据，权重调低，
    - 用新的模型重新训练
    - 重复以上两步，直到终止条件
    - 把每一步训练得到的模型，按照加权平均进行组合在一起
  - 以SVM分类器为例，来演示adaBoost的基本策略
  - 工具：from sklearn.ensemble import AdaBoostClassifier
    - 参数：
      - base:树模型
      - n_estimators：训练次数