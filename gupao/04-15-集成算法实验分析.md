### 1. 集成基本思想
- 测试时，对待测试样本分别通过不同的分类器，汇总最后的结果
- 投票策略：软投票，硬投票
  - 硬投票：少数服从多数，只看每个模型的结果，取出现次数大的作为最终结果
  - 软投票：看每个模型的值所对应的概率值，拿概率值及结果进行加权平均，作为最终结果
    - 要求各个分类器都能得出概率值
- 实验过程：
  - 先选择几个算法：
    - from sklearn.ensemble import VotingClassifier # 投票器
    - from sklearn.ensemble import RandomForestClassifier # 随机森林
    - from sklearn.linear_model import LogisticRegression # 逻辑回归
    - from sklearn.svm import SVC # 支持向量机
  - 各个分类算法实例化
    - RandomForestClassifier()
    - LogisticRegression()
    - SVC()
  - 投票器实例化：VotingClassifier
    - 参数：
      - estimators: 各个分类器对象组成的列表[("分类器的名字", 分类器对象), ("分类器的名字", 分类器对象), ...]
      - voting：投票策略
  - 用投票器实例进行训练
  - 用投票器进行预测
  - 衡量效果
    - from sklearn.metrics import accuracy_score

### 2. Bagging策略
- 算法过程
  - 首先，对训练数据集进行多次采样，保证每次得到的采样数据都是不同的(有放回的)
  - 然后，分别训练多个相同的模型（参数也是相同）
- 工具：
  - from sklearn.ensemble import BaggingClassifier # 集成算法
    - 参数
      - estimator：指定用什么树模型
      - n_estimators：用多少个模型实例
      - bootstrap：是否进行回放的采样
- OOB策略
  - OOB：Out Of Bag——袋外数据
  - BaggingClassifier：指定oob_score=True

### 3. 随机森林
- Bagging算法的典型代表
- 工具：from sklearn.ensenble import RandomForestClassifier
- 特征重要性：
  - RandomForestClassifier().feature_importances_
    - 数值越高，越重要
  - 用热度图更好的展示特征重要性

### 4. Boosting-提升策略
- AdaBoost
  - 样本权重项：每个数据的权重不是相同的
    - 每一轮训练后，没有验证对的数据，权重调高，验证对了的数据，权重调低，
    - 用新的模型重新训练
    - 重复以上两步，直到终止条件
    - 把每一步训练得到的模型，按照加权平均进行组合在一起
  - 以SVM分类器为例，来演示adaBoost的基本策略
  - 工具：from sklearn.ensemble import AdaBoostClassifier
    - 参数：
      - base:树模型
      - n_estimators：训练次数
- Gradient Boosting: 梯度提升决策树
  - 工具包：
    - sklearn.emsemble.GradientBoostingClassifier # 第一代
    - XGBBoost # 第二代
    - lightGBM（第三代，推荐）
      - from lightgbm import LGBMClassifier
  - 参数的建议
    - 学习率要小一些；树的个数要大一些

### 5. 提前停止策略
- 防止结果出现反弹
- 通过参数的传参来达到提前停止的效果
  - 如果连着几次训练，效果都没有得到提升，就可以停止下来
- 参数
  - warm_start：每一次接着上一次的结果进行训练，沿用上一次训练结束的结果

### 6. 堆叠集成（Stacking）
- 步骤
  - 第1步：选取几个分类模型，进行训练(X_train, y_train)
  - 第2步：用训练后的模型，对测试数据集进行预测，得到预测结果，并组装成新的数据集(X_val, y_val) -> X_val_all
  - 第3步：实例化一个新的分类模型
  - 第4步：将第2步的预测结果组成的数据集及结果集，作为新的分类模型的输入，进行训练(X_val_all, y_val)
  - 第5步：用新模型进行预测(X_test, y_test)
