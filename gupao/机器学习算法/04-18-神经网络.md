### 1. 理解深度学习
- 深度学习解决的核心问题：如何提取特征

### 2. 机器学习流程
- 数据获取
- 特征工程：是所有机器学习算法的最核心部分
  - 对于深度学习来说：会自主提取、组合合适的特征
- 建立模型
- 评估与应用

### 3. 特征工程的作用
- 数据特征决定了模型的上限
- 数据的预处理和特征提取是最核心的
- 算法与参数的选择：决定了如何逼近这个上限
- 特征如何选择
  - 传统特征提取方法：每个特征赋予权重值
  - 深度学习：对原始数据做各种变换操作
    - 对原始数据进行学习，并得出结论：什么样的特征是最合适的

### 4. 深度学习的应用
- 90%的应用
  - 计算机视觉
  - 自然语言识别
- 缺点
  - 计算比较慢，因为参数多
- 数据规模：越大越能发挥深度学习的效果

### 5. 图像分类
- 计算机视觉面临的挑战：
  - 部分遮蔽
  - 背景混入 

### 6. K近邻算法
- 计算流程
  - 1. 计算已知类别数据集中的点与当前点的距离
  - 2. 按照距离依次排序
  - 3. 选取与当前点距离最小的K个点
  - 4. 确定前K个点所在类别的出现概率
  - 5. 返回前K个点出现频率最高的类别，作为当前点预测分类
- 数据库样例：CIFAR-10
- 图像距离计算公式
- 为什么K近邻不能用来做图像分类？
  - 背景主导是一个最大的问题，我们关注的主要是主体
- 如何才能让机器学习到哪些是重要的成分？

### 7. 神经网络基础
- 线性函数（得分函数）
  - 从输入到输出的映射：得到每个类别的得分$$f(x, W)=W@x+b$$
    - x：特征
    - W：权重参数（参数个数与特征个数一致）
    - b：偏置项，微调参数（各个类别，各自微调）
- 损失函数：衡量分类的结果
  - 结果的得分值，有着明显的差异；
  - 我们需要明确的指导模型的当前效果：有多好或有多差
  - 正则化惩罚项：损失函数 = 数据损失 + 正则化惩罚项
  - 损失值：$$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)$$
- softmax分类器
  - 将输入得到的得分值，转化为概率值
    - 归一化 $$p_j = \frac{e^{s_j}}{\sum_je^{s_j}}$$ $$s=f(x_j;W)$$
    - 计算损失值：$$L_i=-logP(Y=y_i|X=x_i)$$
- 前向传播：输入到输出的过程，网络结构：输入层->隐藏层->输出层
  - 输入层到隐藏层；隐藏层到输出层；计算损失
    - 1）输入层到隐藏层
      - 线性变换
      - 激活函数
    - 2）隐藏层到输出层
      - 线性变换
      - 激活函数
    - 3）计算损失
    - 激活函数：引入非线性，使网络能够拟合复杂关系
  - 线性函数（计算得分）-> 激活函数（如softmax分类器，得到概率值）->损失函数（计算损失值）
- 反向传播：更新模型的W参数——梯度下降
  - 链式法则：从后往前、逐层优化；
    - 梯度是一步步传播的
  - 门单元
    - 加法门单元：均等分配
    - MAX门单元：梯度只给最大的
    - 乘法门单元：互换的感觉

### 8. 整体架构
- 层次结构：输入层，隐藏层（可以有多层），输出层
- 输入层到隐藏层、隐藏层到下一层隐藏层、隐藏层到输出层：通过权重参数来连接
- 非线性：发生在对每一层与权重参数进行计算之后
- 函数的表示(以MAX门单元举例)
  - 基本结构：$f=W_2max(0, W_1x)$
  - 继续堆叠一层：$f=W_3max(0, W_2max(0, W_1x))$
- 神经元个数对结果的影响
  - 神经元个数越多，效果越好，但过大的话，也存在过拟合的风险
  - 64、128、256、512等都是常见的值
- 目标：找到数据中的规律

### 9. 正则化的作用
- 惩罚力度对结果的影响

### 10. 激活函数
- 非常重要的一部分
- 非线性变换
- 常用的激活函数：
  - Sigmoid：会存在梯度消失的现象
  - Relu：现阶段常用
  - Tanh等

### 11. 数据预处理
- 不同的预处理结果，会使得模型的效果发生很大的差异
- 输入在使用神经网络进行处理之前，都需要先进行预处理

### 12. 参数初始化
- 参数初始化同样非常重要
- 通常，我们采用随机策略来进行参数初始化 $$W = 0.01 * np.random.randn(D, H)$$

### 13. DROP-OUT
- 过拟合是神经网络非常头疼的一个大问题！
- DROP-OUT：
  - 在神经网络的每次训练过程中，在每一层，随机的丢弃一部分神经元
  - 在测试时，就选用全部的神经元
