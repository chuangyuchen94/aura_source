### 1. 算法的实现
- 第1步：初始化参数
  - 层数：神经元的层数
  - 权重参数：初始化（需要考虑偏置项）
  - 最大迭代次数
  - 学习率
- 第2步：数据预处理
  - 标准化
- 第3步：训练数据
  - 梯度下降：实现前向传播、反向传播
    - 计算当前的损失值
    - 计算当前的梯度值
    - 更新参数值
  - 损失函数：计算损失值
    - 前向传播得到最终结果，
    - 与真实值相比，计算损失值
  - 计算梯度值
    - 反向传播，
    - 每一层对结果的影响
    - 计算梯度值

### 2. 总结
- 前向传播、在计算损失值之前，是不知道最终结果的，即，训练的标签值，只会在计算损失值时用到，在训练、前向传播的过程中，并没有用到
- 前向传播结束后，计算得到损失值及梯度值，梯度值用于反向逐层更新参数
  - 输入层到第1层隐藏层：线性变换、sigmoid转换
  - 第1层隐藏层到输出层：线性变换、softmax计算概率值
  - 这种情况下，前向传播的梯度值如何计算，反向传播该如何做
- 链式法则
  - 隐藏层的误差：通过链式法则反向传播（假设包括输入层、输出层，总共有4层，输入层为第1层，输出层为第4层）
    - 输出层误差：输出层误差 = 输出层结果 - 输出层标签值 $$\delta^{(4)}=a^{(4)}-y$$
      - $\delta$：误差
      - $a$：输出层结果（预测值）
    - 反向传播：隐藏层误差 $$\delta^{(3)}={\theta^{(3)}}^T\delta^{(4)}\circ g'(z^{(3)})$$ $$\delta^{(2)}={\theta^{(2)}}^T\delta^{(3)}\circ g'(z^{(2)})$$
      - $\delta^{(l)}$：第$l$层的误差
      - ${(\theta^{(l)})}^T$：权重矩阵的转置
      - $g'(z^{(l)})$：激活函数的导数
        - Sigmoid函数的导数：$$g'(z^{(l)})=g(z)(1-g(z))$$ $$g(z)=\frac{1}{(1+e^{-z})}$$
  - 虽然，每一层到下一层都经过了线性转换和激活函数的处理，然而，在反向传播时，我们并不需要关心反向经过激活函数的梯度是多少，我们只需要关心反向传播到线性回归时的梯度，因为，最终我们要算的是线性转换的梯度、然后用梯度值更新线性转换的参数值
  - 参数更新，是在所有层的梯度计算完成之后，再统一更新的，也就是说，在反向传播计算下一层的梯度时，取当前层的下一层的参数时，必须用当前迭代轮次的旧值，而不是更新后的值（因为在程序实现时，在每一层是可以一边计算梯度，一边更新参数的）
  - `如何理解每一层`
    - 参数归属的层：
      - 只要有输出，这一层就有参数
      - 再接着，有输出，就有预测值；有预测值，就有误差（损失）
      - 有损失，接着就是计算梯度、优化参数
      - 总结一下：
        - 每一层，有神经元个数，参数是连接两个两个层之间的桥梁；
          - 输入层：考虑偏置项；
          - 隐藏层：不考虑偏置项；
        - 每一层，有输入，有参数，有输出（前向传播）；
        - 有误差、有损失，计算梯度，更新参数（反向传播）
    - 输出层就是特殊的一层：只有输入，没有输出，或者干脆说，就没有输出层的存在，就无需定义输出层