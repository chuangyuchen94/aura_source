### 1. 贝叶斯简介
- 解决了“逆概”问题
  - 正向概率
    - 例：假设袋子里有N个白球、M个黑球，摸出黑球的概率
  - 逆向概率
    - 例：事先不知道袋子里黑白球的比例，摸出若干个球后，观察这些球的颜色后，对袋子里的黑白球比例做出推测
- 贝叶斯公式：$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

### 2. 模型比较理论
- `最大似然`：最符合观测数据的最优优势
  - 最符合观测数据（即P(D|h)最大的）
- 奥卡姆剃刀：P(h)较大的模型有较大的优势
  - 先验概率较大的模型，具有较大的优势
- 例：平面上的N个点
  - 可以分别用直线来拟合（模型1），也可以用二阶多项式来拟合（模型2），也可以用三阶多项式来拟合（模型3）；尤其是，可以用N-1阶多项式来完美的通过N个数据点
  - 用“奥卡姆剃刀”来解读：越是高阶的多项式越是不常见
- 例：垃圾邮件过滤实例
  - 假设：
    - D：表示垃圾邮件，由N个单词组成
    - h+：表示垃圾邮件
    - h0：表示正常邮件
  - 则
    - P(h+|D) = P(h+)*P(D|h+)/P(D)
    - P(h-|D) = P(h-)*P(D|h-)/P(D)
  - 计算过程
    - P(h+|D)和P(h-|D)是用来比较大小的，所以，两个式子的P(D)都可以约去不计
    - P(h+)和P(h-)，作为先验概率，只要计算一个邮件库里垃圾邮件和正常邮件的比例就可以求得
    - D里面有N个单词d1, d2, d3, ..., dn，所以
      - P(D|h+) = P(d1|h+) * P(d2|d1, h+) * P(d3|d2, d1, h+)*...
  - `朴素贝叶斯`：假设di与di-1是完全条件无关的，即，特征之间相互独立、互不影响
    - p(D|h+)进一步简化为P(d1|h+)*P(d2|h+)*P(d3|h+)*...
      - 对于P(d1|h+),P(d2|h+),P(d3|h+)...，只要统计di这个单词在垃圾邮件中出现的频率即可
- 例：拼写检查
- 问题：我们看到用户输入了一个不在字典中的单词，我们猜测：他真正想要输入的单词是什么？
- P(我们猜测他想输入的单词|他实际输入的单词)
- 贝叶斯定理很重要的一点：先验概率

### 3. 贝叶斯算法实现
- 垃圾邮件过滤实例
- 概述
  - D表示邮件，由N个单词组成
  - h+：垃圾邮件
  - h-：正常邮件
- 需要解决的两个问题：
  - 遇到出现的词语，既不在垃圾词组中，也不在正常词组中，则P(n|h+)为0，导致整个乘式为0；
    - 平滑处理：概率当做1
  - 概率值每一个都很小，累乘后接近于0
    - 对结果取对数np.log(p)
- 步骤
  - 步骤1：预处理
    - 邮件数据读取
    - 分词
    - 将单词转换为小写
  - 步骤2：创建语料表
  - 步骤3：切分训练集、测试集
  - 步骤4：构建特征向量、标签
  - 步骤5：训练
    - 统计词汇在正常邮件、垃圾邮件中出现的概率
  - 步骤6：预测

### 4. 实现贝叶斯算法：求，给定数据的前提下，某个场景出现的概率
- P(h+|D)：表示垃圾邮件的概率
  - 其中：
    - D是邮件（内容），
    - h+表示垃圾邮件
    - h-表示正常邮件
- 按照贝叶斯算法
  - P(h+|D) = P(D|h+) * P(h+) / P(D)，
  - P(h-|D) = P(D|h-) * P(h-) / P(D)，
   - P(D)可以不用管，毕竟最终比较的是P(h+|D)和P(h-|D)的大小即可，并不需要得出准确值
   - P(h+)、P(h-)是先验概率，只要统计现有的即可
   - D是一封邮件，可以拆成一个个的单词，即，邮件由单词组成，w1, w2, w3..., wn
   - 进一步的，P(D|h+) = P(w1,w2...wn|h+) = P(w1|h+)*P(w2|h+)...P(wn|h+)
   - P(wn|h+)表示：垃圾邮件中，出现单词wn的概率，这里是利用了朴素贝叶斯，假设单词之间的出现的独立的，不存在依赖关系
- 具体的算法实现
  - P(h+)、P(h-)：是基于对训练数据的统计得出——训练数据中，垃圾邮件、正常邮件的比例
  - P(wn|h+)：垃圾邮件中，单词wn出现的比例——统计训练的垃圾邮件中，单词wn出现的比例
  - P(wn|h-)：正常邮件中，单词wn出现的比例——统计训练集的正常邮件中，单词wn出现的比例
  - P(h+|D) = P(D|h+) * P(h+) / P(D) 近似 P(w1|h+)*P(w2|h+)...P(wn|h+) * P(h+)
  - P(h-|D) = P(D|h-) * P(h-) / P(D) 近似 P(w1|h-)*P(w2|h-)...P(wn|h-) * P(h-)
  - 综合P(h+|D)与P(h-|D)：
    - ln(P(hi|D)) = X @ ln(P(Wj|hi)) + ln(P(hi))，
    - i代表+和-；
    - Wj的汇总就是D
    - 例如：
      - X = (50, 844), y = (50,) , 
      - P(Wj|hi) = (844, 1) ==> 垃圾邮件中，词库中的单词出现的概率（频率） 
      - P(hi)为标量(1,)