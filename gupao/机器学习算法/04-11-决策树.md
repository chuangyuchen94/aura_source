### 1. 决策树
- 树模型
  - 决策树：从根节点一步步走到叶子结点
  - 所有数据最终会走到叶子节点
  - 可用于：分类、线性回归
  - 分类的先后顺序，对结果有影响
    - 根节点的分类效果一般需要是最强的
- 树的组成
  - 根节点：第一个选择点
  - 非叶子节点与分支：中间的选择过程
  - 叶子节点：最终的决策结果
- 决策树的训练与测试
  - 训练：从头到尾构建出一棵决策树
    - 从根节点开始选择特征，如何进行特征切分
    - 选择下一级节点的特征，并进行特征切分
    - 一级一级往下构建
- 如何切分特征（选择特征及切分）
  - 关键问题：每一级的特征如何选择，如何切分
  - 目标：通过一种衡量标准，找到每一级最好的特征
  - `衡量标准：熵`
    - 熵：随机变量不确定性的度量。
      - 通俗的理解：混乱程度
      - 不确定性越高，熵值越大
      - 经过分类完之后，熵值的结果
    - `熵值计算公式`：$$H(X)=-\sum_{i \in X}^n p_i * log(p_i), i=1,2,...,n$$
      - 单个分类的概率: $$p_i \in [0, 1]$$ $$log(p_i) \in (-\infty, 0]$$
      - 当$p=0$或$p=1$时，$H(p)=0$，随机变量完全没有随机性
      - 当$p=0.5$时，$H(p)=1$，此时随机变量的不确定性最大
    - 例子：
      - A集合 [1, 1, 1, 1, 1, 1, 1, 1, 2, 2]
      - B结合 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
      - 显然，A集合的熵值要低
    - 在分类中，我们希望通过节点分支后，数据类别的熵值小
  - `信息增益`：表示特征X使得类别Y的不确定性减少的程度，即熵值（类别Y的不确定性）的减少程度
- `基于熵来构建决策树`
  - 首先，要构建根节点。
    - 依据：信息增益
    - 用哪个特征进行分类，信息增益大，就用哪个作为根节点
      - 先计算原始数据的熵值
      - 逐个特征进行分析：特征的每个枚举值所对应的熵值，再对枚举值加权相加，得到特征的熵值： $$\sum_j^mp(x_j)H(x_j)$$
    - 确定根节点之后，在根节点的基础上，再对其余特征、通过信息增益找出二级节点、三级节点等
      - 直到遍历完所有特征，或达到最大迭代次数等终止条件
- 决策树算法
  - ID3：信息增益（无法解决稀疏特征带来的问题）
  - C4.5：信息增益率（解决ID3的问题，考虑自身熵）
    - 将自身熵作为分母，信息增益作为分子
  - CART：适用GINI系数来当做衡量标准
    - GINI系数：$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{i=1}^K p_k^2$$
- 连续值怎么办：选取（连续值的）哪个分界点？
  - 贪婪算法
    - 排序：从小到大排序
    - 对每个点进行切分尝试，计算熵值及信息增益，选择信息增益最大的点
- 决策树剪枝策略
  - 为什么要剪枝：避免过拟合
  - 剪枝策略：预剪枝，后剪枝
    - 预剪枝：边建立决策树，边进行剪枝——更实用
      - 方案：限制深度，限制叶子节点个数，限制叶子节点样本数，限制信息增量等
      - 通过交叉实验来确定最佳方案
    - 后剪枝：当建立完决策树后，再进行剪枝操作
      - 通过一定的衡量标准（叶子节点越多，损失越大）：$$C_\alpha(T)=C(T)+\alpha|T_{leaf}|$$
- 回归的解决：
  - 分类的方法：无法计算熵值，但可以计算方差
  - 预测值的计算：预测结果所在节点的平均数

### 2. 决策树代码实现
- 对决策树的理解
  - 组成：节点，分支，子节点
    - 节点：特征值
    - 分支：对特征的切分逻辑
    - 子节点：处于上个节点的切分逻辑之下，即，标签值已经经过上一级特征切分逻辑的筛选，数据集为经过切分后的数据集；再求解其最优特征值，并继续发展分支及下一级子节点
  - 熵：节点的初始熵值A，节点划分逻辑所对应的熵值B，信息增益值A-B
  - 注意：
    - 每个特征可以分布在不同层级，根据局部最优，计算属于该分支下的最优特征。即，每次分割后的子集独立处理
- 递归函数的调用
- 步骤
  - 步骤1：计算根节点当前的熵值
  - 步骤2：罗列出所有未用到的特征，
  - 针对当前的每个特征，计算其信息增益值，选择信息增益值最大的，作为当前层级的特征
  - 步骤2：去除当前特征

### 3. 集成算法
- 集成：相当于把树模型进行了融合
  - 融合的方法很多
- 几种算法：
  - Bagging：训练多个分类器，取平均 $$f(x)=\frac{1}{M}\sum_{m=1}^Mf_m(x)$$
  - Boosting：提升算法
    - 从弱学习器开始加强，通过加权来进行训练
    - 新加入的一棵树，要比原来的强
  - Stacking：堆叠模型
    - 聚合多个分类或回归模型（可以分阶段来做）
- `Bagging模型`
  - 全称：bootstrap aggregation（并行训练一堆分类器）
  - 最典型的代表：随机森林
    - 森林：多个决策树并行放在一起；每个树各自产生预测结果，汇总取最大概率值（分类）或平均值（回归）
    - 随机：数据随机采样，特征随机选择（让结果产生多样性）
      - 随机是为了保证泛化能力
    - 优势：
      - 能够处理很高维度（很多feature）的数据，并且不用做特征选择
      - 在训练完之后，它能够给出哪些feature比较重要
      - 容易做成并行算法，速度比较快
      - 可以进行可视化展示，便于分析（可解释性强）
  - 适合做集成的模型：树模型，因为可以通过随机让泛化能力变强
    - KNN就不太合适
  - 树的个数
    - 理论上，树越多，效果越好；实际上，超过一定数量后，准确率差不多就上下浮动了
- `Boosting模型`
  - 逐步增加模型：每加一个新的模型，都能让整体效果得到提升
    - 每新加的一棵树，都是为了弥补前面的残差（前面没做好的地方）
    - 最终结果的计算：多棵树的结果进行相加（回归）
  - 公式：(加入一棵树，要比原来强) $$F_m(x)=F_{m-1}(x)+argmin_h\sum_{i=1}^nL(y_i, F_{m-1}(x_i)+ h(x_i))$$
  - 典型代表：
    - AdaBoost
    - Xgboost
  - AdaBoost：会根据前一次的分类效果调整数据权重
    - 如果某一个数据在这次分错了，那么在下一次就会给它更大的权重
    - 最终的结果：每个分类器根据自身的准确性来确定各自的权重，再合体
      - 所有结果求平均
- `Stacking模型`
  - 堆叠：很暴力，拿来一堆直接上
  - 分阶段
    - 第一阶段：选择各种算法，产生结果
    - 第二阶段，将第一阶段的结果，作为第二阶段的输入，产生最终的结果
  - 在竞赛中常见，在实际应用中不常用：因为训练比较耗时