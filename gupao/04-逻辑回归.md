### 1. 单分类
- 逻辑回归是一个分类算法
- 机器学习算法的选择：能用简单的还是先用简单的
  - 越复杂的方法，模型的过拟合风险越大，模型训练起来越复杂
  - 先拿逻辑回归做一个base model，在此基础上再做改进，看一下能提升多少
- 逻辑回归的决策边界：可以是非线性的
- Sigmoid函数
  - 公式：$$g(z)=\frac{1}{1+e^{-z}}$$
    - 自变量取值为任意实数，输出值为[0, 1]
    - 线性回归的预测值：$y=\theta^Tx$ $$h(\theta)=g(y)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$ $$$\theta=\begin{bmatrix} \theta_0 \\ \theta_1 \\ ...\\ \theta_n \\ \end{bmatrix}$, x = $\begin{bmatrix} x_0 \\ x_1 \\ ...\\ x_n \\ \end{bmatrix}$$$
  - 梯度
  - 参数更新

### 2.多分类
- softmax函数：$$$$
- 将多分类任务，拆解成多个二分类任务
- 例如，对存在3种颜色的样本进行分类，总共有100个样本
  - 即，X为[100, 3]的矩阵，$\theta$为[3, 3]的矩阵

### 3.总结
- 所有的分类问题，基本上都可以用逻辑回归来解决

### 4.对逻辑回归的进一步理解
- 逻辑回归不是线性回归的延伸——逻辑回归的线性部分是线性组合，但整体属于分类模型
- 梯度下降是统一应用到整个模型中，而不是分开处理
  - 逻辑回归的梯度下降，是整体优化的过程，包括线性部分和sigmoid的组合
  - 可以用scipy.optimize.minimize函数来替代手动实现的梯度下降

### 5.代码实现
- 工具包：
  - from scipy.optimize import minimize # 替代梯度下降的方法
    - 作用：寻找`目标函数`的最小值
    - 参数：
      - `fun`: 目标函数（需返回标量值）。对于机器学习来说，一般就是损失函数；目标函数的定义：fun(x0, *args), 即，fun(x0, X_train, y_train)
      - `x0`: 初始参数值，即$\theta$。为一维数组(n, )
      - `args`: 传递给目标函数的额外参数。为元组，或可迭代的对象；对于机器学习来说，一般就是训练用的特征数据集（X_train）和标签数据集(y_train)
      - `method`: 优化算法
      - `jac`: 梯度函数，其定义为：jac(x0, *xargs) , 即，jac(x0, X_train, y_train)
      - `options`: 优化选项，包括：最大迭代次数、最大误差、最小误差等。如options={
                    "maxiter": self.max_iter, # 最大迭代次数（防止无限循环）
                    "gtol": 1e-6, # 梯度范数的容忍度（当梯度小于此值时终止优化）
                    "disp": True, # 是否打印优化过程信息},
      - `callback`: 回调函数。在每次迭代后都会被调用，用于输出当前迭代的参数值。callback(xk), 其中，xk是当前参数向量，包含了两个属性x（迭代后的参数值）和fun
        - def callback(xk):
         print(f"Iteration parameter: {xk}")
    - 结果
      - `success`: 优化是否成功
      - `message`: 优化或终止的原因描述
      - `x`：最优参数向量（即最终的 θ）。
      - `fun`：目标函数在最优点的值（即最小损失值）。
      - `nfev`：目标函数调用次数（评估优化效率）。
    - 注意：
      - minimize最终是求出最优参数$\theta$，minimize内部已经实现了迭代更新参数值，无需手工的显式更新$\theta$
      - 对特征值进行标准化，可以加快迭代优化的过程，但需要做归一化处理
- 算法组成
  - i. 线性组合：得到线性输出 $$y = X\theta$$
  - ii. sigmoid函数：将线性组合的输出值，映射到[0,1]
    - 线性回归的预测值：$$y=X\theta$$其中，$X$为[m, n]的矩阵，$\theta$为[n, 1]的矩阵
    - 将预测值进行映射：$$p=h(\theta)=g(y)=g(\theta^Tx)=\frac{1}{1+e^{-X\theta}}$$
  - iii. 分类决策：根据概率$p$的阈值将样本分为类别0或1
  - 损失函数：$$J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}L(h_{\theta}(x^{(i)}), y^{(i)})=-\frac{1}{m}[Y^Tlog(h_\theta(X))+(1-Y)^Tlog(1-h_\theta(X))]$$
    - 对逻辑回归的“损失”的理解：属于正确类别的概率值，与“1”（正确类别的实际值为1）的偏差
    - 损失由两部分组成：
      - 正确类别的概率值与“1”（正确类别的实际值为1）的偏差，
      - 错误类别的概率值与“0”（正确类别的实际值为0）的偏差
  - 梯度计算：$$\nabla J(\theta)=\frac{1}{m}X^T(h_\theta(X)-Y)$$
  - 参数更新：$\theta_j:=\theta_j-\alpha\frac{1}{m}X^T(h_\theta(X)-Y)$
- 算法实现步骤
  - 1. 读取数据，做数据准备

### 6.决策边界
