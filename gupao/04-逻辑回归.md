### 1. 单分类
- 逻辑回归是一个分类算法
- 机器学习算法的选择：能用简单的还是先用简单的
  - 越复杂的方法，模型的过拟合风险越大，模型训练起来越复杂
  - 先拿逻辑回归做一个base model，在此基础上再做改进，看一下能提升多少
- 逻辑回归的决策边界：可以是非线性的
- Sigmoid函数
  - 公式：$$g(z)=\frac{1}{1+e^{-z}}$$
    - 自变量取值为任意实数，输出值为[0, 1]
    - 线性回归的预测值：$y=\theta^Tx$ $$h(\theta)=g(y)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$ $$$\theta=\begin{bmatrix} \theta_0 \\ \theta_1 \\ ...\\ \theta_n \\ \end{bmatrix}$, x = $\begin{bmatrix} x_0 \\ x_1 \\ ...\\ x_n \\ \end{bmatrix}$$$
  - 梯度
  - 参数更新

### 2.多分类
- softmax函数：$$$$
- 将多分类任务，拆解成多个二分类任务
- 例如，对存在3种颜色的样本进行分类，总共有100个样本
  - 即，X为[100, 3]的矩阵，$\theta$为[3, 3]的矩阵

### 3.总结
- 所有的分类问题，基本上都可以用逻辑回归来解决

### 4.对逻辑回归的进一步理解
- 逻辑回归不是线性回归的延伸——逻辑回归的线性部分是线性组合，但整体属于分类模型
- 梯度下降是统一应用到整个模型中，而不是分开处理
  - 逻辑回归的梯度下降，是整体优化的过程，包括线性部分和sigmoid的组合
  - 可以用scipy.optimize.minimize函数来替代手动实现的梯度下降

### 5.代码实现
- 工具包：
  - from scipy.optimize import minimize # 梯度下降的方法
- 算法组成
  - i. 线性组合：得到线性输出 $$y = \theta^TX$$
  - ii. sigmoid函数：将线性组合的输出值，映射到[0,1]
    - 线性回归的预测值：$$y=X\theta$$其中，$X$为[m, n]的矩阵，$\theta$为[n, 1]的矩阵
    - 将预测值进行映射：$$p=h(\theta)=g(y)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$
  - iii. 分类决策：根据概率$p$的阈值将样本分为类别0或1
  - 损失函数：$$J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}L(h_{\theta}(x^{(i)}), y^{(i)})=-\frac{1}{m}[Y^Tlog(h_\theta(X))+(1-Y)^Tlog(1-h_\theta(X))]$$
    - 对逻辑回归的“损失”的理解：属于正确类别的概率值，与“1”（正确类别的实际值为1）的偏差
    - 损失由两部分组成：
      - 正确类别的概率值与“1”（正确类别的实际值为1）的偏差，
      - 错误类别的概率值与“0”（正确类别的实际值为0）的偏差
  - 梯度计算：$$\nabla J(\theta)=\frac{1}{m}X^T(h_\theta(X)-Y)$$
  - 参数更新：$\theta_j:=\theta_j-\alpha\frac{1}{m}X^T(h_\theta(X)-Y)$
- 算法实现步骤
  - 1. 读取数据，做数据准备