### 1. 交叉验证
- 把训练集分成几份
- 每次训练时，把其中某一份（或几份）作为验证集，其他作为训练集
  - 每次都建立新的模型
  - 拿多次的验证结果平均，作为模型的评分
- 代码示例：直接用一整套的方法
  - from sklearn.model_selection import cross_val_score
- 代码示例：自己决定每一步怎么做
  - from sklearn.model_selection import StratifiedKFold # 切分数据
  - from sklearn.base import clone # 以相同的参数克隆模型

### 2. 混淆矩阵（Confusion Matrix）
- 评估模型性能时，不能仅仅用一个指标
- 关键指标：(F开头都代表做错了的；一个完美的分类器，应该只有T开头的不为零)
  - `TP`: true positives
  - `FP`: false positives
  - `FN`: false negatives
  - `TN`: true negatives
- `混淆矩阵`：$$
\begin{bmatrix}
TN & FP \\
FN & TN
\end{bmatrix}
$$
- 代码示例
  - from sklearn.metrics import confusion_matrix
- 其他指标
  - `精度`：$precision = \frac{TP}{TP+FP}$
  - `召回率`：$recall = \frac{TP}{TP+FN}$
- 代码示例：
  - from sklearn.metrics import precision_score # 精度
  - from sklearn.metrics import recall_score # 召回率
- `F1 score`：将Precision和Recall结合到一起，形成一个指标，调和平均值给予低值更多权重 $$F1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}}=\frac{TP}{TP+\frac{FN+FP}{2}}$$
- 代码示例
  - from sklearn.metrics import f1_score
- `阈值`对结果的影响
  - 得到每个实例的分数，再由设置的阈值来对分数大小判断，以进行预测
  - 随着阈值的提高，精度变大、召回率变小；另外，精度和召回率是无法同时增大的
  - 代码示例：生成不同阈值下的精度和召回率
    - from sklearn.metrics import precision_recall_curve
  - ROC曲线：二元分类中的常用评估方法
    - 绘制TPR（true positive rate）和FPT（false positive rate）的曲线
      - $TPR=\frac{TP}{TP+FN}$
      - $FPR=\frac{FP}{FP+TN}$
    - 一个好的分类器，应该是TPR尽可能大，且同时FPR尽可能小
    - 比较分类器的方法：测量曲线下面积（AUC）
      - 完美分类器的ROC AUC等于1，而纯随机分类器的ROC AUC等于0.5
    - 代码示例
      - from sklearn.metrics import roc_curve
      - from sklearn.metrics import roc_auc_score