### 1. 任务目标
- 数据：信用卡交易数据: creditcard.csv
  - 数据已进行脱敏
  - 最后一列是分类：哪些是正常交易，哪些是异常交易
- 对任务的理解：属于分类任务
- 任务流程
- 1）加载数据，观察问题
  - 数据不是干净的：有缺失值，有异常值
- 2）针对问题给出解决方案
  - 对所有问题，逐一列出解决方案，然后再统一处理，因为数据量如果很大（上T），处理一次的时间会很长
- 3）数据集切分
  - 为了之后做评估
- 4）评估方法对比：评估方法很多
- 5）逻辑回归模型
- 6）建模结果分析
- 7）方案效果对比
  - 目的：得到更有结果
  - 可能建立多个模型
  - 调整参数

### 2. 加载数据，观察问题
- 导入工具包
  - import pandas as pd
  - import numpy as np
  - import matplotlib.pyplot as plt
- 数据读取
  - data = pd.read_csv(数据文件)
  - data.head() # 显示前5行数据
    - 关注：
      - 数据类型
      - 字符型编码的处理
      - 数据是否有问题
        - 不同特征之间，数据的量级是否差别较大
    - 标签（y）的特点
      - 分布情况：画图
        - count_classes = pd.value_counts(data["Class"], sort=True).sort_index()
        - count_classes.plot(kind = "bar")
        - plt.title()
        - plt.xlabel()
        - plt.ylabel()
        - plt.show()
- `数据分析`：分析特征值和标签值
  - 哪些列是数值型数据，哪些列是枚举值型数据
  - 对于数值型数据
    - 每一列：最大值，最小值，平均值，标准差
  - 对于枚举型数据
    - 枚举值的分布
  - 通用的分析
    - 是否存在缺失值
- 挑战与解决方案
  - 数据样本不均衡
    - 方案1：over sample（`过采样`），让不同标签类型的数据样本一样多
    - 方案2：under sample（`下采样`），让数量多的标签的数据，减少到跟数量少的一样多
  - 具体用哪个方案，没有经验值可以参考，需要都进行实验，以观察效果来定

### 3. 数据标准化处理
- 什么是数据标准化
  - 让不同取值范围的数据，变成取值范围一样
  - 公式：$$\frac{x-\mu}{\sigma}$$
    - $\mu$：平均值  # 中心化
    - $\sigma$：标准差  # 缩放
  - 工具
    - from sklearn.preprocessing import StandardScaler
    - scaler = StandardScaler()
    - scaler.fit_transform(data)
- 取出没意义的数据
  - 比如：每一行的序列，或类似序列的值

### 4. 下采样方案
- 划分特征数据、标签数据
- 得到所有异常数据样本的索引
- 得到所有正常数据样本的索引
- 在正常数据样本中，随机取出跟异常样本相同数量的数据
- 将正常数据和异常数据的索引合并，取出下采样数据集

### 5. 数据集划分
