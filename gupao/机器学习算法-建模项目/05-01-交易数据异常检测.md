### 1. 任务目标
- 数据：信用卡交易数据: creditcard.csv
  - 数据已进行脱敏
  - 最后一列是分类：哪些是正常交易，哪些是异常交易
- 对任务的理解：属于分类任务
- 任务流程
- 1）加载数据，观察问题
  - 数据不是干净的：有缺失值，有异常值
- 2）针对问题给出解决方案
  - 对所有问题，逐一列出解决方案，然后再统一处理，因为数据量如果很大（上T），处理一次的时间会很长
- 3）数据集切分
  - 为了之后做评估
- 4）评估方法对比：评估方法很多
- 5）逻辑回归模型
- 6）建模结果分析
- 7）方案效果对比
  - 目的：得到更优结果
  - 可能建立多个模型
  - 调整参数

### 2. 加载数据，观察问题
- 导入工具包
  - import pandas as pd
  - import numpy as np
  - import matplotlib.pyplot as plt
- 数据读取
  - data = pd.read_csv(数据文件)
  - data.head() # 显示前5行数据
    - 关注：
      - 数据类型
      - 字符型编码的处理
      - 数据是否有问题
        - 不同特征之间，数据的量级是否差别较大
    - 标签（y）的特点
      - 分布情况：画图
        - count_classes = pd.value_counts(data["Class"], sort=True).sort_index()
        - count_classes.plot(kind = "bar")
        - plt.title()
        - plt.xlabel()
        - plt.ylabel()
        - plt.show()
- `数据分析`：分析特征值和标签值
  - 哪些列是数值型数据，哪些列是枚举值型数据
  - 对于数值型数据
    - 每一列：最大值，最小值，平均值，标准差
  - 对于枚举型数据
    - 枚举值的分布
  - 通用的分析
    - 是否存在缺失值
- 挑战与解决方案
  - 数据样本不均衡
    - 方案1：over sample（`过采样`），让不同标签类型的数据样本一样多
    - 方案2：under sample（`下采样`），让数量多的标签的数据，减少到跟数量少的一样多
  - 具体用哪个方案，没有经验值可以参考，需要都进行实验，以观察效果来定

### 3. 数据标准化处理
- 什么是数据标准化
  - 让不同取值范围的数据，变成取值范围一样
  - 公式：$$\frac{x-\mu}{\sigma}$$
    - $\mu$：平均值  # 中心化
    - $\sigma$：标准差  # 缩放
  - 工具
    - from sklearn.preprocessing import StandardScaler
    - scaler = StandardScaler()
    - scaler.fit_transform(data)
- 取出没意义的数据
  - 比如：每一行的序列，或类似序列的值

### 4. 下采样方案
- 划分特征数据、标签数据
- 得到所有异常数据样本的索引
- 得到所有正常数据样本的索引
- 在正常数据样本中，随机取出跟异常样本相同数量的数据
- 将正常数据和异常数据的索引合并，取出下采样数据集

### 5. 数据集划分
- 训练集、验证集、测试集
- 交叉验证
  - from sklearn.cross_validation import train_test_split
- 对整个数据集进行划分：测试集用于测试
- 对下采样数据集进行划分：训练集用于训练

### 6. 建模
- 评估方法：根据目标选择评估方法
  - 召回率（Recall）：(预测出的值中，关注类别的个数) / 关注的类别的个数
    - TP
    - FP
    - FN
    - TN
    - Recall = TP / (TP + FN)
      - recall高，自然就是最好的
    - 工具：
      - from sklearn.metrics import recall_score
- 交叉验证
  - 工具：
    - from sklearn.cross_validation import KFold, cross_val_score
- 正则化惩罚项
  - 正则化惩罚力度：C = [0.01, 0.1, 1, 10, 100]
  - 惩罚方法
- 展示结果用的表格：pd.DataFrame
- 逻辑回归模型
  - from sklearn.linear_model import LogisticRegression
  - model = LogisticRegression(C, penalty="l1")
- 展示结果：召回率
  - 交叉验证的每次的召回率
  - 平均召回率
- 两个核心的关键：模型参数，特征
- 挑选最好的参数
  - recall高，自然就是最好的
- 用最好的参数，重新训练模型

### 7. 混淆矩阵评估分析（画图）
- 绘制混淆矩阵（模板，用的时候直接复制；不用自己手写绘图代码）

### 8. 下采样方案在原始数据集中的结果及遇到的问题
- 下采样的训练数据集太小，导致：召回率还行，但误杀率太高

### 9. 阈值对结果的影响
- 先用最好的参数进行建模
- 指定不同的阈值：threshold = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
- 得到预测结果的概率值：predict_proba

### 10. SMOTE过采样方案
- SMOTE样本生成策略
  - from imblearn.over_sampling import SMOTE
  - 找出少数样本
  - 对每个样本与其他样本之间的距离进行排序
    - 先遍历一个点：
      - 算出这个点与其他点的距离
      - 对距离进行排序
      - 生成新样本：指定点 + 距离的随机数（0，1）
  - 指定倍率：生成多少倍的少数样本，即，找多少个临近的点
  - 