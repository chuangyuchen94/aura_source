### 0. 总结

### 1. 任务概述
- 数据来源：http://www.sogou.com/labs/resource/ca.php
- 整体流程
  - 新闻数据集处理
    - 是爬虫爬取的数据，需要先对文本进行预处理才能使用
  - 文本分词
    - 通常，我们处理的都是词，而不是一篇文章
  - 去停用词
    - 停用词会对结果产生不好的影响，需要剔除掉
  - 构建文本特征
    - 需要构建成数值特征，才能执行分类任务
    - 如何构建合适特征，是自然语言处理中最重要的一步
    - 两种基本的方法
    - 工具包
      - import jieba

### 2. 读取数据
- 因为原始数据是爬虫爬下来的，有些不整洁，需要先进行清洗
- 指定列名

### 3. 中文分词
- 将一篇文章转换成一个list
- 使用jieba进行分词
  - 对每一篇文章进行分词：jieba.lcut

### 4. 去停用词
- 停用词表
  - 通常来说，词频越高，越可能是停用词
  - 停用词表，可以自己做，也可以找现成的
- 过滤停用词

### 5. 可视化展示
- 先得到词，及每个词的词频
- 工具：词云
  - from wordcloud import WordCloud

### 6. 提取关键词
- TF-IDF
  - TF：词频
  - IDF：逆文档频率
    - 如果一个词，这也出现，那也出现，那就在这篇文章中，可能就没那么重要
    - 如果一个词，只在当前文章中大量出现，那么，这个词对于这篇文章来说，就可能比较重要
- 工具包
  - import jieba.analyse
- 步骤
  - 第1步，把之前的分词及过滤后的结果，组合成一个句子，用""来join
  - 第2步，提取关键词
    - jieba.analyse.extract_tags
    - 参数
      - topK：提取几个关键词

### 7. 制作数据集标签
- 对标签进行编码，变成数值

### 8. 构建特征（词袋模型特征）
- 所有输入，特征必须是相同的大小
- 特征：语料库的大小是不变的，无论是什么文章
  - 从现有的数据中，构建语料库
- 先用少量的数据进行测试，而不是用原始的大数据
  - 无论是做什么操作，这个思路都适用
  - 分析结果，是否与预期一致，再评估这种做法是否可行
  - 数据量太大，不好对结果进行分析
- 词袋模型：把文本转换成特征向量
  - 每一篇文章就是一行样本，其中的每个词，就对应特征的一个列
  - 对文章中的词出现的次数进行统计
- 工具
  - from sklearn.feature_extraction.text import CountVectorizer
- 词袋模型的缺陷
  - 问题1：没有考虑词与词之间的先后顺序；
  - 问题2：容易得到稀疏矩阵：不是每个词都会出现；且特征的维度会过大
  - 问题3：不能很好的处理同义词、近义词
- 改进方法：
  - ngram模型：
    - 在初始化CountVectorizer时，设置ngram_range参数，将词连在一起，构成先后顺序
  - 设置CountVectorizer的参数max_features：指定最大特征数
    - 按词频大小，保留前max_features个最常出现的词
    - 先做停用词的过滤

### 9. 使用词袋模型的特征来建模，观察结果
- 多分类任务
- 模型：贝叶斯模型
- 工具
  - from sklearn.naive_bayes import MultinomialNB

### 10. 制作TF-IDF特征
- 工具
  - from sklearn.feature_extraction.text import TfidfVectorizer

### 11. 使用TF-IDF特征建模，观察结果

### 12. 词向量模型
- 配合神经网络的递归模型来使用
