### 1. 决策树
- 树模型
  - 决策树：从根节点一步步走到叶子结点
  - 所有数据最终会走到叶子节点
  - 可用于：分类、线性回归
  - 分类的先后顺序，对结果有影响
    - 根节点的分类效果一般需要是最强的
- 树的组成
  - 根节点：第一个选择点
  - 非叶子节点与分支：中间的选择过程
  - 叶子节点：最终的决策结果
- 决策树的训练与测试
  - 训练：从头到尾构建出一棵决策树
    - 从根节点开始选择特征，如何进行特征切分
    - 选择下一级节点的特征，并进行特征切分
    - 一级一级往下构建
- 如何切分特征（选择特征及切分）
  - 关键问题：每一级的特征如何选择，如何切分
  - 目标：通过一种衡量标准，找到每一级最好的特征
  - `衡量标准：熵`
    - 熵：随机变量不确定性的度量。
      - 通俗的理解：混乱程度
      - 不确定性越高，熵值越大
      - 经过分类完之后，熵值的结果
    - `熵值计算公式`：$$H(X)=-\sum_{i \in X}^n p_i * log(p_i), i=1,2,...,n$$
      - 单个分类的概率: $$p_i \in [0, 1]$$ $$log(p_i) \in (-\infty, 0]$$
      - 当$p=0$或$p=1$时，$H(p)=0$，随机变量完全没有随机性
      - 当$p=0.5$时，$H(p)=1$，此时随机变量的不确定性最大
    - 例子：
      - A集合 [1, 1, 1, 1, 1, 1, 1, 1, 2, 2]
      - B结合 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
      - 显然，A集合的熵值要低
    - 在分类中，我们希望通过节点分支后，数据类别的熵值小
  - `信息增益`：表示特征X使得类别Y的不确定性减少的程度，即熵值（类别Y的不确定性）的减少程度
- `基于熵来构建决策树`
  - 首先，要构建根节点。
    - 依据：信息增益
    - 用哪个特征进行分类，信息增益大，就用哪个作为根节点
      - 先计算原始数据的熵值
      - 逐个特征进行分析：特征的每个枚举值所对应的熵值，再对枚举值加权相加，得到特征的熵值： $$\sum_j^mp(x_j)H(x_j)$$
    - 确定根节点之后，在根节点的基础上，再对其余特征、通过信息增益找出二级节点、三级节点等
      - 直到遍历完所有特征，或达到最大迭代次数等终止条件
- 决策树算法
  - ID3：信息增益（无法解决稀疏特征带来的问题）
  - C4.5：信息增益率（解决ID3的问题，考虑自身熵）
    - 将自身熵作为分母，信息增益作为分子
  - CART：适用GINI系数来当做衡量标准
    - GINI系数：$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{i=1}^K p_k^2$$
- 连续值怎么办：选取（连续值的）哪个分界点？
  - 贪婪算法
    - 排序：从小到大排序
    - 对每个点进行切分尝试，计算熵值及信息增益，选择信息增益最大的点
- 决策树剪枝策略
  - 为什么要剪枝：避免过拟合
  - 剪枝策略：预剪枝，后剪枝
    - 预剪枝：边建立决策树，边进行剪枝——更实用
      - 方案：限制深度，限制叶子节点个数，限制叶子节点样本数，限制信息增量等
      - 通过交叉实验来确定最佳方案
    - 后剪枝：当建立完决策树后，再进行剪枝操作
      - 通过一定的衡量标准（叶子节点越多，损失越大）：$$C_\alpha(T)=C(T)+\alpha|T_{leaf}|$$
- 回归的解决：
  - 分类的方法：无法计算熵值，但可以计算方差
  - 预测值的计算：预测结果所在节点的平均数

### 2. 决策树代码实现
- 递归函数的调用
- 